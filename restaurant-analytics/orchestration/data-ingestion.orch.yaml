type: "orchestration"
version: "1.0"
pipeline:
  components:
    Data Ingestion Start:
      type: "start"
      transitions:
        unconditional:
        - "Initialize File Registry"
      parameters:
        componentName: "Data Ingestion Start"
    Initialize File Registry:
      type: "run-transformation"
      transitions:
        success:
        - "File Processing Iterator"
      parameters:
        componentName: "Initialize File Registry"
        transformationJob: "restaurant-analytics/transformation/file-registry.tran.yaml"
        setScalarVariables:
        setGridVariables:
      postProcessing:
        updateOutputMessage: "ðŸ“‹ File registry initialized with processing priorities\
          \ and quality thresholds - 4 restaurant data sources configured for intelligent\
          \ ingestion"
        updateScalarVariables:
    File Processing Iterator:
      type: "table-iterator"
      transitions:
        success:
        - "Create Ingestion Log Table"
      iterationTarget: "Dynamic Table Creator"
      parameters:
        componentName: "File Processing Iterator"
        mode: "Basic"
        database: "[Environment Default]"
        schema: "[Environment Default]"
        targetTable: "file_list"
        concurrency: "Sequential"
        columnMapping:
        - - "file_name"
          - "file_name"
        - - "table_name"
          - "table_name"
        orderBy:
        sort: "Ascending"
        breakOnFailure: "No"
    Dynamic Table Creator:
      type: "sql-executor"
      parameters:
        componentName: "Dynamic Table Creator"
        scriptLocation: "Component"
        declareSqlVariables: "Include all"
        sqlScript: "-- Dynamic Restaurant Data Ingestion with Schema Inference\n--\
          \ Drop existing table to ensure clean creation\nDROP TABLE IF EXISTS ${table_name};\n\
          \n-- Create table with inferred schema from staged file\nCREATE TABLE ${table_name}\
          \ \nUSING TEMPLATE (\n  SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*)) \n  FROM TABLE(\n\
          \    INFER_SCHEMA(\n      LOCATION => '@${stage_name}/${file_name}',\n \
          \     FILE_FORMAT => '${file_format}'\n    )\n  )\n);\n\n-- Load data into\
          \ the newly created table with comprehensive error handling\nCOPY INTO ${table_name}\n\
          FROM '@${stage_name}/${file_name}'\nFILE_FORMAT = (TYPE = 'CSV' FIELD_DELIMITER\
          \ = ',' SKIP_HEADER = 1 RECORD_DELIMITER = '\\n')\nON_ERROR = 'SKIP_FILE'\n\
          RETURN_FAILED_ONLY = TRUE;\n\n-- Enhanced logging for file-level processing\n\
          CREATE TABLE IF NOT EXISTS restaurant_processing_log (\n  pipeline_run_id\
          \ VARCHAR(100),\n  file_name VARCHAR(255),\n  table_name VARCHAR(255),\n\
          \  rows_loaded INTEGER,\n  errors_seen INTEGER,\n  first_error VARCHAR(500),\n\
          \  processing_timestamp TIMESTAMP,\n  status VARCHAR(50)\n);\n\nINSERT INTO\
          \ restaurant_processing_log (\n  pipeline_run_id,\n  file_name,\n  table_name,\n\
          \  rows_loaded,\n  errors_seen,\n  first_error,\n  processing_timestamp,\n\
          \  status\n)\nSELECT \n  'INGEST_' || TO_VARCHAR(CURRENT_TIMESTAMP(), 'YYYYMMDD_HH24MISS'),\n\
          \  '${file_name}',\n  '${table_name}',\n  COUNT(*),\n  0,\n  CASE \n   \
          \ WHEN COUNT(*) = 0 THEN 'File skipped due to data quality issues'\n   \
          \ ELSE NULL\n  END,\n  CURRENT_TIMESTAMP(),\n  CASE \n    WHEN COUNT(*)\
          \ > 0 THEN 'SUCCESS'\n    ELSE 'FILE_SKIPPED'\n  END\nFROM ${table_name};\n"
      postProcessing:
        updateOutputMessage: "âœ… Data ingestion completed for ${file_name} â†’ ${table_name}\
          \ table created with schema validation and quality checks"
        updateScalarVariables:
    Create Ingestion Log Table:
      type: "create-table-v2"
      transitions:
        success:
        - "Log Pipeline Completion"
      parameters:
        componentName: "Create Ingestion Log Table"
        createMethod: "Create If Not Exists"
        database: "[Environment Default]"
        schema: "[Environment Default]"
        table: "restaurant_ingestion_log"
        snowflakeTableType: "Permanent"
        columns:
        - - "pipeline_run_id"
          - "VARCHAR"
          - "50"
          - ""
          - ""
          - "No"
          - "No"
          - "Pipeline execution identifier"
          - ""
        - - "total_files_processed"
          - "NUMBER"
          - ""
          - ""
          - ""
          - "No"
          - "No"
          - "Number of files successfully processed"
          - ""
        - - "completion_timestamp"
          - "TIMESTAMP"
          - ""
          - ""
          - ""
          - "No"
          - "No"
          - "When the pipeline completed"
          - ""
        - - "status"
          - "VARCHAR"
          - "20"
          - ""
          - ""
          - "No"
          - "No"
          - "Pipeline completion status"
          - ""
        - - "pipeline_version"
          - "VARCHAR"
          - "10"
          - ""
          - ""
          - "No"
          - "No"
          - "Version of the pipeline"
          - ""
        defaultDdlCollation:
        primaryKeys:
        clusteringKeys:
        dataRetentionTimeInDays:
        comment: "Log table for restaurant data ingestion pipeline completion tracking"
      postProcessing:
        updateOutputMessage: "ðŸ“‹ Ingestion log table ready for completion tracking"
        updateScalarVariables:
    Restaurant Analytics Pipeline:
      type: "run-transformation"
      transitions:
        success:
        - "Executive Dashboard Analytics"
      parameters:
        componentName: "Restaurant Analytics Pipeline"
        transformationJob: "restaurant-analytics/transformation/business-analytics.tran.yaml"
        setScalarVariables:
        setGridVariables:
      postProcessing:
        updateOutputMessage: "ðŸ“Š Comprehensive business analytics completed - 7 KPI\
          \ tables generated: CLV, AOV, Menu Performance, Employee Performance, Customer\
          \ Retention, Seasonal Analysis, Master Dataset"
        updateScalarVariables:
    Executive Dashboard Analytics:
      type: "run-transformation"
      parameters:
        componentName: "Executive Dashboard Analytics"
        transformationJob: "restaurant-analytics/transformation/executive-dashboard.tran.yaml"
        setScalarVariables:
        setGridVariables:
      postProcessing:
        updateOutputMessage: "ðŸŽ¯ Executive dashboard analytics completed - C-suite\
          \ ready summaries and strategic insights generated"
        updateScalarVariables:
    Log Pipeline Completion:
      type: "run-transformation"
      transitions:
        success:
        - "Restaurant Analytics Pipeline"
      parameters:
        componentName: "Log Pipeline Completion"
        transformationJob: "restaurant-analytics/transformation/ingestion-completion-logger.tran.yaml"
        setScalarVariables:
        setGridVariables:
      postProcessing:
        updateOutputMessage: "ðŸŽ¯ Data ingestion pipeline completed successfully -\
          \ 4 restaurant tables validated and ready for business intelligence processing"
        updateScalarVariables:
  variables:
    file_name:
      metadata:
        type: "TEXT"
        description: "Current file being processed from restaurant data stage"
        scope: "COPIED"
        visibility: "PUBLIC"
      defaultValue: ""
    stage_name:
      metadata:
        type: "TEXT"
        description: "Snowflake stage containing restaurant CSV files"
        scope: "SHARED"
        visibility: "PUBLIC"
      defaultValue: "mat_pro.resturant"
    table_name:
      metadata:
        type: "TEXT"
        description: "Target table name for current file processing"
        scope: "COPIED"
        visibility: "PUBLIC"
      defaultValue: ""
    file_format:
      metadata:
        type: "TEXT"
        description: "File format definition for CSV parsing"
        scope: "SHARED"
        visibility: "PUBLIC"
      defaultValue: "mat_pro.infer_fmt"
design:
  components:
    Data Ingestion Start:
      position:
        x: -220
        "y": 70
      tempMetlId: 1
    Initialize File Registry:
      position:
        x: -40
        "y": 70
      tempMetlId: 2
    File Processing Iterator:
      position:
        x: 310
        "y": 50
      tempMetlId: 3
    Dynamic Table Creator:
      position:
        x: 310
        "y": 50
      tempMetlId: 4
    Create Ingestion Log Table:
      position:
        x: 700
        "y": 70
      tempMetlId: 5
    Restaurant Analytics Pipeline:
      position:
        x: 1120
        "y": 70
      tempMetlId: 6
    Executive Dashboard Analytics:
      position:
        x: 1300
        "y": 70
      tempMetlId: 7
    Log Pipeline Completion:
      position:
        x: 830
        "y": 70
      tempMetlId: 8
  notes:
    "1":
      position:
        x: -300
        "y": -280
      size:
        height: 480
        width: 380
      theme: "light-blue"
      content: |
        ### **ðŸ“‹ File Registry & Control**

        **Purpose:** Central management for restaurant data processing

        **Files:** 4 CSV sources (customers, employees, menus, orders)
        **Mapping:** Each CSV â†’ corresponding Snowflake table
        **Features:** Processing priorities, quality thresholds, error handling

        **Benefits:** Centralized control and configuration management
    "2":
      position:
        x: 140
        "y": -280
      size:
        height: 480
        width: 380
      theme: "yellow"
      content: "### **âš¡ Parallel Processing Engine**\n\n**Strategy:** Concurrent file\
        \ processing for maximum performance\n\n**Features:**\n- Concurrent processing\
        \ of all 4 files simultaneously\n- Dynamic SQL with INFER_SCHEMA for auto-detection\
        \  \n- COPY INTO with comprehensive error handling\n- Audit logging and quality\
        \ validation\n\n**Performance:** 4x faster execution with parallel processing\n"
    "3":
      position:
        x: 570
        "y": -280
      size:
        height: 480
        width: 330
      theme: "light-green"
      content: |
        ### **âœ… Validation & Completion**

        **Quality Checks:**
        - Verifies all 4 tables created successfully
        - Logs completion status and metrics
        - Creates audit trail for monitoring

        **Output:** Clean, validated data ready for analytics
    "4":
      position:
        x: 930
        "y": -280
      size:
        height: 480
        width: 360
      theme: "green"
      content: |
        ### **ðŸ“Š Analytics Integration**

        **Business Analytics Pipeline:**
        - 8 KPI tables (CLV, AOV, retention, performance)
        - AI-powered customer intelligence with Cortex
        - Seasonal analysis and trend detection

        **Executive Dashboards:**
        - C-suite ready summaries and insights
        - Real-time performance metrics
        - Strategic decision-making data
