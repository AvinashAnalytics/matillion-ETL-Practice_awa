type: "orchestration"
version: "1.0"
pipeline:
  components:
    Data Ingestion Start:
      type: "start"
      transitions:
        unconditional:
        - "Initialize File Registry"
      parameters:
        componentName: "Data Ingestion Start"
    Initialize File Registry:
      type: "run-transformation"
      transitions:
        success:
        - "File Processing Iterator"
      parameters:
        componentName: "Initialize File Registry"
        transformationJob: "restaurant-analytics/transformation/file-registry.tran.yaml"
        setScalarVariables:
        setGridVariables:
      postProcessing:
        updateOutputMessage: "üìã File registry initialized with processing priorities\
          \ and quality thresholds - 4 restaurant data sources configured for intelligent\
          \ ingestion"
        updateScalarVariables:
    File Processing Iterator:
      type: "table-iterator"
      transitions:
        success:
        - "Ingestion Completion Logger"
      iterationTarget: "Dynamic Table Creator"
      parameters:
        componentName: "File Processing Iterator"
        mode: "Basic"
        database: "[Environment Default]"
        schema: "[Environment Default]"
        targetTable: "file_list"
        concurrency: "Sequential"
        columnMapping:
        - - "file_name"
          - "file_name"
        - - "table_name"
          - "table_name"
        orderBy:
        sort: "Ascending"
        breakOnFailure: "Yes"
    Dynamic Table Creator:
      type: "sql-executor"
      parameters:
        componentName: "Dynamic Table Creator"
        scriptLocation: "Component"
        declareSqlVariables: "Include all"
        sqlScript: "-- Restaurant Data Ingestion: ${file_name}\n-- Dynamic table creation\
          \ with schema inference\n\nBEGIN\n  -- Step 1: Create ingestion log table\
          \ if it doesn't exist\n  CREATE TABLE IF NOT EXISTS restaurant_ingestion_log\
          \ (\n    pipeline_run_id VARCHAR(100),\n    total_files_processed INTEGER,\n\
          \    completion_timestamp TIMESTAMP,\n    status VARCHAR(50),\n    pipeline_version\
          \ VARCHAR(10)\n  );\n\n  -- Step 2: Create table with inferred schema\n\
          \  CREATE OR REPLACE TABLE ${table_name}\n  USING TEMPLATE (\n    SELECT\
          \ ARRAY_AGG(OBJECT_CONSTRUCT(*))\n    FROM TABLE(\n      INFER_SCHEMA(\n\
          \        LOCATION => '@${stage_name}/${file_name}',\n        FILE_FORMAT\
          \ => '${file_format}'\n      )\n    )\n  );\n\n  -- Step 3: Add metadata\
          \ comment with processing info\n  ALTER TABLE ${table_name} \n  SET COMMENT\
          \ = 'Restaurant data: ${file_name} - Auto-generated on ' || CURRENT_TIMESTAMP()\
          \ || ' - Status: SUCCESS';\n\n  -- Step 4: Load data with comprehensive\
          \ error handling\n  COPY INTO ${table_name}\n  FROM '@${stage_name}/${file_name}'\n\
          \  FILE_FORMAT = (\n    TYPE = 'CSV' \n    SKIP_HEADER = 1 \n    FIELD_OPTIONALLY_ENCLOSED_BY\
          \ = '\"'\n    TRIM_SPACE = TRUE\n    ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE\n\
          \    EMPTY_FIELD_AS_NULL = TRUE\n  )\n  ON_ERROR = 'CONTINUE'\n  PURGE =\
          \ FALSE;\n\n  -- Step 5: Log successful load\n  INSERT INTO restaurant_ingestion_log\
          \ (\n    pipeline_run_id,\n    total_files_processed,\n    completion_timestamp,\n\
          \    status,\n    pipeline_version\n  )\n  VALUES (\n    'FILE_' || '${file_name}'\
          \ || '_' || TO_VARCHAR(CURRENT_TIMESTAMP(), 'YYYYMMDD_HH24MISS'),\n    1,\n\
          \    CURRENT_TIMESTAMP(),\n    'FILE_SUCCESS',\n    '1.0'\n  );\n\nEXCEPTION\n\
          \  WHEN OTHERS THEN\n    -- Create error log table if it doesn't exist\n\
          \    CREATE TABLE IF NOT EXISTS restaurant_error_log (\n      error_id VARCHAR(100),\n\
          \      file_name VARCHAR(100),\n      error_message VARCHAR(500),\n    \
          \  error_timestamp TIMESTAMP\n    );\n    \n    -- Log error details\n \
          \   INSERT INTO restaurant_error_log VALUES (\n      'ERR_' || TO_VARCHAR(CURRENT_TIMESTAMP(),\
          \ 'YYYYMMDD_HH24MISS'),\n      '${file_name}',\n      SQLERRM,\n      CURRENT_TIMESTAMP()\n\
          \    );\n    \n    -- Re-raise the error to stop processing\n    RAISE;\n\
          END;\n"
      postProcessing:
        updateOutputMessage: "‚úÖ Data ingestion completed for ${file_name} ‚Üí ${table_name}\
          \ table created with schema validation and quality checks"
        updateScalarVariables:
    Ingestion Completion Logger:
      type: "sql-executor"
      transitions:
        success:
        - "Restaurant Analytics Pipeline"
      parameters:
        componentName: "Ingestion Completion Logger"
        scriptLocation: "Component"
        declareSqlVariables: "Include all"
        sqlScript: "-- Restaurant Data Ingestion Completion Log\nCREATE TABLE IF NOT\
          \ EXISTS restaurant_ingestion_log (\n  pipeline_run_id VARCHAR(50),\n  total_files_processed\
          \ INTEGER,\n  completion_timestamp TIMESTAMP,\n  status VARCHAR(20),\n \
          \ pipeline_version VARCHAR(10)\n);\n\n-- Final pipeline completion with\
          \ validation\nDECLARE\n  table_count INTEGER := 0;\nBEGIN\n  SELECT COUNT(*)\
          \ INTO table_count\n  FROM information_schema.tables \n  WHERE table_name\
          \ IN ('REST_CUSTOMERS', 'REST_EMPLOYEES', 'REST_MENUS', 'REST_ORDER')\n\
          \  AND table_schema = CURRENT_SCHEMA();\n  \n  -- Log completion with validation\
          \ results\n  INSERT INTO restaurant_ingestion_log (\n    pipeline_run_id,\n\
          \    total_files_processed,\n    completion_timestamp,\n    status,\n  \
          \  pipeline_version\n  )\n  VALUES (\n    'PIPELINE_' || TO_VARCHAR(CURRENT_TIMESTAMP(),\
          \ 'YYYYMMDD_HH24MISS'),\n    table_count,\n    CURRENT_TIMESTAMP(),\n  \
          \  CASE WHEN table_count = 4 THEN 'SUCCESS' ELSE 'PARTIAL_SUCCESS' END,\n\
          \    '1.0'\n  );\nEND;\n"
      postProcessing:
        updateOutputMessage: "üéØ Data ingestion pipeline completed successfully -\
          \ 4 restaurant tables validated and ready for business intelligence processing"
        updateScalarVariables:
    Restaurant Analytics Pipeline:
      type: "run-transformation"
      transitions:
        success:
        - "Executive Dashboard Pipeline"
      parameters:
        componentName: "Restaurant Analytics Pipeline"
        transformationJob: "restaurant-analytics/transformation/business-analytics.tran.yaml"
        setScalarVariables:
        setGridVariables:
      postProcessing:
        updateOutputMessage: "üìä Business analytics processing completed - 8 KPI tables\
          \ generated including advanced seasonal and quality analytics"
        updateScalarVariables:
    Executive Dashboard Pipeline:
      type: "run-transformation"
      parameters:
        componentName: "Executive Dashboard Pipeline"
        transformationJob: "restaurant-analytics/transformation/executive-dashboard.tran.yaml"
        setScalarVariables:
        setGridVariables:
      postProcessing:
        updateOutputMessage: "üíº Executive dashboard processing completed - Real-time\
          \ KPIs and executive summaries ready for C-suite consumption"
        updateScalarVariables:
  variables:
    file_name:
      metadata:
        type: "TEXT"
        description: "Current file being processed from restaurant data stage"
        scope: "SHARED"
        visibility: "PUBLIC"
      defaultValue: ""
    stage_name:
      metadata:
        type: "TEXT"
        description: "Snowflake stage containing restaurant CSV files"
        scope: "SHARED"
        visibility: "PUBLIC"
      defaultValue: "mat_pro.resturant"
    table_name:
      metadata:
        type: "TEXT"
        description: "Target table name for current file processing"
        scope: "SHARED"
        visibility: "PUBLIC"
      defaultValue: ""
    file_format:
      metadata:
        type: "TEXT"
        description: "File format definition for CSV parsing"
        scope: "SHARED"
        visibility: "PUBLIC"
      defaultValue: "mat_pro.infer_fmt"
design:
  components:
    Data Ingestion Start:
      position:
        x: 0
        "y": 0
      tempMetlId: 1
    Initialize File Registry:
      position:
        x: 190
        "y": 0
      tempMetlId: 2
    File Processing Iterator:
      position:
        x: 440
        "y": -20
      tempMetlId: 3
    Dynamic Table Creator:
      position:
        x: 440
        "y": -20
      tempMetlId: 4
    Ingestion Completion Logger:
      position:
        x: 780
        "y": 0
      tempMetlId: 5
    Restaurant Analytics Pipeline:
      position:
        x: 1120
        "y": 0
      tempMetlId: 6
    Executive Dashboard Pipeline:
      position:
        x: 1490
        "y": 0
      tempMetlId: 7
  notes:
    "1":
      position:
        x: -200
        "y": -381
      size:
        height: 500
        width: 300
      theme: "light-blue"
      content: "### **üöÄ Data Ingestion Control**\n\n**Source Files (4):**\n- rest_menus.csv\
        \ ‚Üí REST_MENUS\n- rest_orders.csv ‚Üí REST_ORDER  \n- rest_customers.csv ‚Üí REST_CUSTOMERS\n\
        - rest_employees.csv ‚Üí REST_EMPLOYEES\n\n**Quality Assurance:**\n- File registry\
        \ validation\n- Schema inference automation\n- Comprehensive error logging\n"
    "2":
      position:
        x: 290
        "y": -381
      size:
        height: 500
        width: 300
      theme: "light-green"
      content: |
        ### **‚öôÔ∏è Dynamic Processing Engine**

        **Iterator Capabilities:**
        - Processes each file automatically
        - Creates tables with inferred schema
        - Handles data loading with error recovery
        - Breaks on failure for data integrity
        - Logs individual file processing status
    "3":
      position:
        x: 670
        "y": -376
      size:
        height: 495
        width: 280
      theme: "yellow"
      content: |
        ### **üìä Completion Validation**

        **Quality Checks:**
        - Validates all 4 tables created
        - Logs pipeline completion status
        - Creates audit trail for monitoring

        **Error Recovery:**
        - Exception handling in SQL
        - Error logging to dedicated table
        - Detailed failure reporting
    "4":
      position:
        x: 1020
        "y": -372
      size:
        height: 490
        width: 590
      theme: "green"
      content: |
        ### **üîÑ Analytics Automation**

        **Business Intelligence:**
        - Runs business analytics pipeline
        - Generates 5 KPI tables
        - Creates executive dashboards
        - Provides C-suite insights

        **End-to-End:** Complete data platform ready for decision making
